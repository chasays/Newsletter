---
layout: post
title: "智能计算系统第二章神经网络基础"
subtitle: 'AICS'
author: "叉叉敌"
header-style: text
tags:
  - 智能计算系统
---

## 浅层神经网络特点 

需要数据量小、训练快，其局限性在于对复杂函数的表示能力有限，针对复杂分类问题其泛化能力受到制约 Why Not Go Deeper? Kurt Hornik证明了理论上两层神经网络 足以拟合任意函数，过去也没有足够的数据和计算能力


##  深度神经网络的成功:ABC 

深度神经网络不断发展不仅依赖于自身的结构优势，也依赖于如下 一些外在因素 
- Alqorithm:算法日新月异，优化算法层出不穷(学习算法- >BP算法-> Pre-training, Dropout等方法) 
- Big data:数据量不断增大(10->10k->100M) 
- Computing:处理器计算能力的不算提升(晶体管->CPU - >集群/GPU ->智能处理器)


##  神经网络训练 

`正向传播`(推断)是根据输入，经过权重、激活函数计算出隐层，将输 入的特征向量从低级特征逐步提取为抽象特征，直到得到最终输出结果 的过程。

![](https://gitee.com/chasays/mdPic/raw/master/uPic/20220207193348.png)


`反向传播`是根据正向传播的输出结果和期望值计算出损失函数，再通过链 式求导，最终从网络后端逐步修改权重使输出和期望值的差距变到最小的 过程。

![](https://gitee.com/chasays/mdPic/raw/master/uPic/20220207193520.png)

## 实例

![](https://gitee.com/chasays/mdPic/raw/master/uPic/20220207193558.png)


反向传播的作用是将神经网络的输出误差反向 传播到神经网络的输入端，并以此来更新神经 网络中各个连接的权重的权重 

当第一次反向传播法完成后，网络的模型参数 得到更新，网络进行下一轮的正向传播过程， 如此反复的迭代进行训练，从而不断缩小计算 值与真实值之间的误差。

##  隐层的设计: 

目前有autoML的专门来找有几个隐藏几点。

隐层节点的作用是提取输入特征中的隐藏规律，每个节 点都赋予一定权重 隐层节点数太少，则网络从样本中获取信息的能力就越 差，无法反映数据集的规律;隐层节点数太多，则网络 的拟合能力过强，可能拟合数据集中的噪声部分，导致 模型泛化能力变差。

##  选择合适的激活函数  (当前的不好用，就换一个)

在神经元中，输入的数据通过加权求和后，还被作用了一个函数G，这个函 数G就是激活函数(Activation Function)。 
激敫活函数给神经元引入了非线性因素，使得神经网络可以任意逼近任何非线 性函数，因此神经网络可以应用到众多的非线性模型中 


激活函数需具备的性质 :

`可微性`:当优化方法是基于梯度的时候，这个性质是必须的。

`输出值的范围`:当激活函数输出值是有限的时候，基于梯度的优化方法会更加稳定， 因为特征的表示受有限权值的影响更显著;当激活函数的输出是无限的时候，模型的 训练会更加高效，不过在这种情况小，一般需要更小的学习率。


##  sigmoid函数 

数学表达式 几何图像 1 o(x)= 1+e-x > 

Sigmoid是最常见的非线性激活函数 

能够把输入的连续实值变换为0和1之 间的输出;如果是非常大的负数，那 么输出就变为0;如果是非常大的正 数，输出就变为1。

非0均值的输出，导致w计算的梯 度始终都是正的 

计算机进行指数运算速度慢 

饱和性问题及梯度消失现象

试试手气换其他函数

##  tanh函数

与sigmoid相比，tanh是0均值的 

在输入很大或是很小的时候，输出几乎 平滑，梯度很小，不利于权重更新


##  ReLU函数

ReLU能够在x>0时保持梯度不衰减，从而缓解 梯度消失问题 

ReLU死掉。如果学习率很大，反向传播后的参 数可能为负数，导致下一轮正向传播的输入为 负数。当输入是负数的时候，ReLU是完全不被 激活的，这就表明一旦输入到了负数， ReLU就 会死掉 

输出范围是无限的


还有其他的PReLU/Leaky ReLu函数，除了激活函数，还是损失函数


##  常用的损失函数

神经网络中损失函数的特性 > 

同一个算法的损失函数不是唯一的 
损失函数是参数(w,b)的函数 
损失函数可以评价网络模型的好坏，损失函数越小说明模型和参 
数越符合训练样本(x,y) 
损失函数是一个标量 
选择损失函数时，挑选对参数(w,b)可微的函数(全微分存在，偏导数一定存在)
损失函数又称为代价函数、目标函数


##  过拟合与正则化

![](https://gitee.com/chasays/mdPic/raw/master/uPic/20220207200618.png)


机器学习不仅要求数据在训练集上求得一个较小的误差，在测试集 上也要表现好。因为模型最终是要部署到没有见过训练数据的真实 场景。提升模型在测试集上的预测效果叫做`泛化`。 

`过拟合 `(overfitting)指模型过度接近训练的数据，模型的泛化能 力不足。具体表现为在训练数据集上测试的误差很低，但在验证数 据集上的误差很大。 

神经网络的层数增加，参数也跟着增加，表示能力大幅度增强，极 容易出现过拟合现象。 

参数范数惩罚、稀疏化、Bagging集成、Dropout、提前终止、数 据集扩增等正则化方法可以有效抑制过拟合

##  交叉验证


交叉验证的方式给每个样本作为测试集和训练集的机会，充分利用 样本信息，保证了鲁棒性，防止过度拟合。 


选择多种模型进行训练时，使用交叉验证能够评判各模型的鲁棒性。


##  小结


`从机器学习到神经网络` 
掌握单变量线性回归、多变量线性回归、感知机、神经元、激活函 数、偏置等的概念和相关用法 

`正向传播、反向传播` 了解数据正向传播和反向传播的过程。

`激活函数` 了解常用激活函数的优缺点 

`损失函数` 了解损失函数的作用和种类、掌握至少一种验证方式。 

`过拟合与正则化` 了解过拟合出现的原因并掌握防止过拟合的正则化方法 

`交叉验证` 掌握至少一种交叉验证的原理和方法